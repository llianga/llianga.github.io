<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Anqi Liang</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Anqi Liang</name>
              </p>
              <p>Hi! I am Anqi Liang. I am a senior PhD student in Computer Science and Technology at <a href="https://en.sjtu.edu.cn/">Shanghai Jiao Tong University</a> advised by <a href="https://www.cs.sjtu.edu.cn/~yaobin/">Prof. Bin Yao</a>. Prior to this, I got my B.E. degree in Computer Science and Technology from <a href="http://eweb.ouc.edu.cn/">Ocean University of China</a>.
              My research interests mainly include <b>data management, spatio-temporal data mining, vector data management, and deep learning<b>.<p>
              </p>
              <p style="text-align:center">
                <a href="lianganqi@sjtu.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=ktpANSIAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/aq.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/aq.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am deeply interested in data mining, spatio-temporal data management, vector data management, and database systems. My goal is to apply artificial intelligence to address challenges in spatio-temporal data management and to leverage these advancements for practical applications, such as smart transportation and smart cities.
              </p>
            </td>
          </tr>
        </tbody></table>


      <!-- News section -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <ul>
              <li><strong>2025-05:</strong> Our paper PGTuner: An Efficient Framework for Automatic and Transferable Configuration Tuning of Proximity Graphs</a> was accepted by SIGMOD!</li>
              <li><strong>2024-12:</strong> Our paper <a href="https://arxiv.org/abs/2412.02448">UNIFY: Unified Index for Range Filtered Approximate Nearest Neighbors Search</a> was accepted by VLDB!</li>
              <li><strong>2024-09:</strong> Our paper <a href="https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=ktpANSIAAAAJ&citation_for_view=ktpANSIAAAAJ:u-x6o8ySG0sC">CLMTR: A Generic Framework for Contrastive Multi-modal Trajectory Representation Learning</a> was accepted by GeoInformatica!</li>
              <li><strong>2024-01:</strong> Our paper <a href="https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=ktpANSIAAAAJ&citation_for_view=ktpANSIAAAAJ:u5HHmVD_uO8C">Sub-trajectory Clustering with Deep Reinforcement Learning</a> was accepted by VLDBJ!</li>

            </ul>
          </td>
        </tr>
      </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <heading>Publications</heading>
                <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/hsig.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2405.10292" id="MultiMon">
                <papertitle> UNIFY: Unified Index for Range Filtered Approximate Nearest Neighbors Search
                </papertitle>
              </a>
              <br>
                <strong>Anqi Liang</strong>, Pengcheng Zhang, Bin Yao, Zhongpu Chen, Yitong Song, Guangxu Cheng
               <br>
              <em> VLDB 2025</em>
              <br>
                [<a href="data/Hybridhnsw.pdf">pdf</a>][<a href="https://arxiv.org/abs/2412.02448">Paper Link</a>]
              <p></p>
              <p>We present UNIFY, an efficient and scalable framework for Range Filtered Approximate Nearest Neighbors Search (RF-ANNS) over high-dimensional vectors with attribute values. UNIFY introduces SIG, a Segmented Inclusive Graph that partitions the dataset by attribute values, ensuring that the PG of any segment combination is a sub-graph of SIG. This enables efficient hybrid filtering by reconstructing and searching a PG from relevant segments. We also propose HSIG, a hierarchical variant of SIG inspired by HNSW, which achieves logarithmic hybrid filtering complexity. By integrating skip list connections and compressed HNSW edges, we enhance HSIG with pre- and post-filtering. Experimental results demonstrate that UNIFY outperforms state-of-the-art methods across small, medium, and large query ranges. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/subtraj.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <a href="https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=ktpANSIAAAAJ&citation_for_view=ktpANSIAAAAJ:u5HHmVD_uO8C" id="MultiMon">
                <papertitle> Sub-trajectory Clustering with Deep Reinforcement Learning
                </papertitle>
                </a>
              <br>
              <strong>Anqi Liang</strong>,
              Bin Yao,
              Bo Wang,
              Yinpei Liu,
              Zhida Chen,
              Jiong Xie,
              Feifei Li
              <br>
              <em>The VLDB Journal 2024 </em><span style="color: red;"></span>
              <br>

               [<a href="data/sub-traj.pdf">pdf</a>] [<a href="https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=ktpANSIAAAAJ&citation_for_view=ktpANSIAAAAJ:u5HHmVD_uO8C">Paper Link</a>]

              <p></p>
              <p>We propose a novel sub-trajectory clustering framework that uses clustering results to guide the segmentation, which is based on reinforcement learning (RL). The key novelty lies in the close cooperation between the segmentation and clustering components, which continuously improve each other to yield better clustering results. We model the trajectory segmentation process as a Markov decision process and apply DQN learning to train an RL model for segmentation, achieving excellent clustering results. Experimental results on real datasets demonstrate the superior performance of the proposed RL-based approach compared to state-of-the-art methods.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/clmtr.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=ktpANSIAAAAJ&citation_for_view=ktpANSIAAAAJ:u-x6o8ySG0sC">
                <papertitle>CLMTR: A Generic Framework for Contrastive Multi-modal Trajectory Representation Learning
                </papertitle>
              </a>
              <br>
              <strong>Anqi Liang</strong>, Bin Yao, Jiong Xie, Wenli Zheng, Yanyan Shen, Qiqi Ge
              <br>
              <em>GeoInformatica 2024 </em><span style="color: red;"></span>
              <br>
                [<a href="data/clmtr.pdf">pdf</a>][<a href="https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=ktpANSIAAAAJ&citation_for_view=ktpANSIAAAAJ:u-x6o8ySG0sC">Paper Link</a>]
              <p></p>
              <p>We propose a generic Contrastive Learning-based Multi-modal Trajectory Representation framework, termed CLMTR. Specifically, we incorporate intra- and inter-trajectory con- trastive learning components to capture the correlations among diverse modal features and the intricate relationships among trajectories, obtaining generic and semantically enriched trajectory representations. We develop multi-modal feature embedding and attention-based fusion approaches to capture the multi-modal characteristics and adaptively obtain the uni- fied embeddings. Experimental results on two real-world datasets demonstrate the superior performance of CLMTR over state-of-the-art methods in three downstream tasks.</p>
            </td>
          </tr>








        <style>
          .sub-table {
              width: 0;
              height: 0;
              overflow: hidden;
              position: absolute;
              left: -9999px;
              opacity: 0;
              visibility: hidden;
          }
      </style>

      <table class="sub-table" align="center">
          <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=y35-AqSkeLIkce_C13W-97DGULFZQWj5YJB3rNARabY&cl=ffffff&w=a"></script>
      </table>

        </tbody></table>




      </td>
    </tr>
  </table>
</body>

</html>
